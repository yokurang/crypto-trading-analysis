y = "Cumulative PnL (US$)"
) +
theme_minimal()
cat("Total Cumulative PnL: ", tail(merged_data$cumulative_pnl, 1))
cat("\n")
cat("Duration of Trading: ", tail(merged_data$timestamp, 1) - head(merged_data$timestamp, 1))
classification.data <- merged_data
# encode the dataset to model what factors are more likely to influence buying
classification.data$side <- ifelse(classification.data$side == "B", 1, 0)
classification.data$side <- as.factor(classification.data$side)
# One-hot encode liquidity
classification.data <- dummy_cols(classification.data,
select_columns = c("liquidity"),
remove_first_dummy = TRUE,
remove_selected_columns = TRUE
)
# Convert liquidity_Taker to numeric so we can use it easily later
classification.data$liquidity_Taker <- as.numeric(
as.character(classification.data$liquidity_Taker)
)
# volatility calculations
volatility_data <- market_data %>%
mutate(mid_price = (bid_prc + ask_prc) / 2) %>%
arrange(timestamp) %>%
mutate(volatility = rollapply(mid_price, width = 20, FUN = sd, fill = NA, align = "right"))
setDT(classification.data)
setDT(volatility_data)
setkey(classification.data, timestamp)
setkey(volatility_data, timestamp)
classification.data <- volatility_data[, .(timestamp, volatility)][classification.data, roll = Inf]
classification.data <- classification.data %>%
arrange(timestamp) %>%
mutate(
balance_lag1 = lag(balance),
spread_lag1 = lag(ask_prc - bid_prc),
mid_price_lag1 = lag((bid_prc + ask_prc) / 2),
volatility_lag1 = lag(volatility)
)
# Fill Size (no lag, trade-specific)
classification.data <- classification.data %>%
mutate(
fill_size = fill_qty * fill_prc # do not encode buy/sell sign
)
# Deviation from Mean Inventory Balance (lagged)
mean_inventory_balance <- mean(classification.data$balance, na.rm = TRUE)
classification.data <- classification.data %>%
mutate(
deviation_from_mean_balance_lag1 = balance_lag1 - mean_inventory_balance,
deviation_from_mean_balance_diff_lag = c(NA, diff(deviation_from_mean_balance_lag1))
)
# Deviation from Mean Market Price (lagged)
mean_mid_price <- mean(classification.data$mid_price, na.rm = TRUE)
classification.data <- classification.data %>%
mutate(
deviation_from_mean_mid_price_lag1 = mid_price_lag1 - mean_mid_price,
deviation_from_mean_mid_price_diff = c(NA, diff(deviation_from_mean_mid_price_lag1))
)
# Lagged Trade PnL and Cumulative PnL
classification.data <- classification.data %>%
mutate(
trade_pnl_lag1 = lag(trade_pnl),
cumulative_pnl_lag1 = lag(cumulative_pnl)
)
classification.data <- na.omit(classification.data) # Remove resulting NAs
str(classification.data)
numerical.variables <- c(
"volatility_lag1",
"spread_lag1",
"fill_prc",
"fill_qty",
"trade_pnl_lag1",
"cumulative_pnl_lag1",
"fee",
"balance_lag1",
"mid_price_lag1",
"fill_size",
"deviation_from_mean_balance_diff_lag",
"deviation_from_mean_mid_price_lag1",
"liquidity_Taker"
)
target.variables <- c("side")
classification.features <- unique(c(numerical.variables, target.variables))
print(classification.features)
classification.variables <- as.data.frame(classification.data[, ..numerical.variables])
classification.target <- classification.data %>% select(all_of(target.variables))
classification.selected <- cbind(
classification.variables,
classification.target
)
# Initial 80/20 split (train+val vs. test)
train_val_index <- createDataPartition(classification.selected$side, p = 0.8, list = FALSE)
train_val_data <- classification.selected[train_val_index, ]
test_data <- classification.selected[-train_val_index, ]
# Split 80% further into training (75%) and validation (25%) -> results in 60/20/20 overall
train_index <- createDataPartition(train_val_data$side, p = 0.75, list = FALSE)
train_data <- train_val_data[train_index, ]
val_data <- train_val_data[-train_index, ]
cat("Training Set Size:", nrow(train_data), "\n")
cat("Validation Set Size:", nrow(val_data), "\n")
cat("Test Set Size:", nrow(test_data), "\n")
# Remove known multicollinear features
classification.features <- setdiff(
classification.features,
c("mid_price_lag1", "balance_lag1")
)
# Separate predictors from the target
classification.predictors <- setdiff(classification.features, "side")
cat("Target Variable:", "side\n")
cat("Predictor Variables: ")
cat(paste(classification.predictors, collapse = ", "))
evaluate_model <- function(feature_set, method_name = NA) {
formula_str <- paste("side ~", paste(feature_set, collapse = " + "))
formula_obj <- as.formula(formula_str)
model <- glm(formula_obj, data = train_data, family = "binomial")
probs <- predict(model, newdata = test_data, type = "response")
roc_obj <- suppressMessages(roc(test_data$side, probs))
auc_val <- as.numeric(roc_obj$auc)
opt_thresh <- coords(roc_obj, x = "best", ret = "threshold", transpose = FALSE)
preds <- ifelse(probs > opt_thresh$threshold, 1, 0)
preds <- factor(preds, levels = levels(test_data$side))
cm <- confusionMatrix(preds, test_data$side)
return(data.frame(
subset = paste(feature_set, collapse = " + "),
num_vars = length(feature_set),
aic = AIC(model),
bic = BIC(model),
accuracy = cm$overall["Accuracy"],
balanced_accuracy = cm$byClass["Balanced Accuracy"],
roc_auc = auc_val,
stringsAsFactors = FALSE
))
}
best.subset.results <- data.frame(
subset = character(),
num_vars = integer(),
aic = numeric(),
bic = numeric(),
accuracy = numeric(),
balanced_accuracy = numeric(),
roc_auc = numeric(),
stringsAsFactors = FALSE
)
# Exhaustive subset search using combinations
for (k in 1:length(classification.predictors)) {
combos <- combinations(
n = length(classification.predictors), r = k,
v = classification.predictors
)
for (i in 1:nrow(combos)) {
vars <- combos[i, ]
result_row <- evaluate_model(vars)
best.subset.results <- rbind(best.subset.results, result_row)
}
}
best_by_aic <- best.subset.results %>%
arrange(aic) %>%
slice(1)
best_by_bic <- best.subset.results %>%
arrange(bic) %>%
slice(1)
best_by_balacc <- best.subset.results %>%
arrange(desc(balanced_accuracy)) %>%
slice(1)
best_by_auc <- best.subset.results %>%
arrange(desc(roc_auc)) %>%
slice(1)
print_model_summary <- function(title, row) {
cat(paste0(title, "\n"))
cat("  Formula: side ~", row$subset, "\n")
cat("  Num Vars:", row$num_vars, "\n")
cat("  Balanced Accuracy:", round(row$balanced_accuracy, 4), "\n")
cat("  Accuracy:", round(row$accuracy, 4), "\n")
cat("  AIC:", round(row$aic, 2), "\n")
cat("  BIC:", round(row$bic, 2), "\n")
cat("  ROC AUC:", round(row$roc_auc, 4), "\n\n")
}
print_model_summary("Best Subset Model by AIC", best_by_aic)
print_model_summary("Best Subset Model by BIC", best_by_bic)
print_model_summary("Best Subset Model by Balanced Accuracy", best_by_balacc)
print_model_summary("Best Subset Model by ROC AUC", best_by_auc)
x_logistic <- as.matrix(train_data[, classification.predictors])
y_logistic <- train_data$side
lasso_logistic <- cv.glmnet(
x = x_logistic, y = y_logistic,
family = "binomial", alpha = 1, nfolds = 10
)
best_lambda_logistic <- lasso_logistic$lambda.min
lasso_coef <- coef(lasso_logistic, s = best_lambda_logistic)
selected_lasso_features <- rownames(lasso_coef)[which(lasso_coef != 0)]
selected_lasso_features <- setdiff(selected_lasso_features, "(Intercept)")
lasso_summary <- evaluate_model(selected_lasso_features, "LASSO")
print_model_summary("LASSO Feature Selection", lasso_summary)
rfe_ctrl <- rfeControl(functions = caretFuncs, method = "cv", number = 10)
rfe_model <- suppressWarnings(
rfe(
x = train_data[, classification.predictors],
y = train_data$side,
sizes = 1:length(classification.predictors),
rfeControl = rfe_ctrl,
method = "glm",
family = "binomial"
)
)
selected_rfe_features <- predictors(rfe_model)
rfe_summary <- evaluate_model(selected_rfe_features, "RFE")
print_model_summary("ðŸ“Œ RFE Feature Selection", rfe_summary)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lubridate)
library(arrow)
library(data.table)
library(caret)
library(pROC)
library(h2o)
library(magrittr)
library(tinytex)
library(readxl)
library(resampledata)
library(car)
library(corrplot)
library(gridExtra)
library(zoo)
library(rpart)
library(rpart.plot)
library(tseries)
library(forecast)
library(lmtest)
library(strucchange)
library(trend)
library(sandwich)
library(TSA)
library(Metrics)
library(gbm)
library(glmnet)
library(fastDummies)
library(reshape2)
library(leaps)
library(torch)
library(randomForest)
library(dplyr)
library(gtools)
set.seed(123)
market_data <- read_parquet("data/market_data.parq")
fills_data <- read_parquet("data/fills_data.parq")
head(fills_data, 3)
head(market_data, 3)
fills_data <- fills_data %>%
mutate(
side = as.factor(side),
liquidity = as.factor(liquidity),
symbol = as.factor(symbol),
exch = as.factor(exch),
fee_ccy = as.factor(fee_ccy)
)
market_data <- market_data %>%
mutate(
symbol = as.factor(symbol)
)
cat("--- Summary of fills_data ---\n")
cat("Dimensions: ", paste(dim(fills_data), collapse = " x "), "\n\n")
str(fills_data)
cat("\n--- Summary of market_data ---\n")
cat("Dimensions: ", paste(dim(market_data), collapse = " x "), "\n\n")
str(market_data)
cat("total NAs in fills_data: ", sum(is.na(fills_data)), "\n")
cat("total NAs in market_data: ", sum(is.na(market_data)))
setDT(fills_data)
setDT(market_data)
setkey(fills_data, timestamp)
setkey(market_data, timestamp)
# selects the most recent (previous) value
merged_data <- market_data[fills_data, roll = Inf]
merged_data <- subset(merged_data, select = -c(i.symbol))
head(merged_data, 5)
cat("--- Summary of merged_data ---\n")
cat("Dimensions: ", paste(dim(merged_data), collapse = " x "), "\n")
cat("Column Names:\n")
colnames(merged_data)
cat("total NAs in merged_data: ", sum(is.na(merged_data)))
merged_data <- merged_data %>% mutate(
mid_price = (bid_prc + ask_prc) / 2
)
calculate_trade_pnl <- function(merged_data) {
merged_data <- merged_data %>%
mutate(
q_i = if_else(side == "B", fill_qty, -fill_qty),
) %>%
mutate(
trade_pnl = (q_i * mid_price) - (q_i * fill_prc)
)
return(merged_data)
}
merged_data <- calculate_trade_pnl(merged_data)
merged_data <- merged_data %>%
mutate(
cumulative_pnl = cumsum(trade_pnl)
)
ggplot(merged_data, aes(x = timestamp, y = cumulative_pnl)) +
geom_line(color = "blue") +
geom_point(color = "red") +
labs(
title = "Cumulative PnL Over Time",
x = "Timestamp",
y = "Cumulative PnL (US$)"
) +
theme_minimal()
cat("Total Cumulative PnL: ", tail(merged_data$cumulative_pnl, 1))
cat("\n")
cat("Duration of Trading: ", tail(merged_data$timestamp, 1) - head(merged_data$timestamp, 1))
classification.data <- merged_data
# encode the dataset to model what factors are more likely to influence buying
classification.data$side <- ifelse(classification.data$side == "B", 1, 0)
classification.data$side <- as.factor(classification.data$side)
# One-hot encode liquidity
classification.data <- dummy_cols(classification.data,
select_columns = c("liquidity"),
remove_first_dummy = TRUE,
remove_selected_columns = TRUE
)
# Convert liquidity_Taker to numeric so we can use it easily later
classification.data$liquidity_Taker <- as.numeric(
as.character(classification.data$liquidity_Taker)
)
# volatility calculations
volatility_data <- market_data %>%
mutate(mid_price = (bid_prc + ask_prc) / 2) %>%
arrange(timestamp) %>%
mutate(volatility = rollapply(mid_price, width = 20, FUN = sd, fill = NA, align = "right"))
setDT(classification.data)
setDT(volatility_data)
setkey(classification.data, timestamp)
setkey(volatility_data, timestamp)
classification.data <- volatility_data[, .(timestamp, volatility)][classification.data, roll = Inf]
classification.data <- classification.data %>%
arrange(timestamp) %>%
mutate(
balance_lag1 = lag(balance),
spread_lag1 = lag(ask_prc - bid_prc),
mid_price_lag1 = lag((bid_prc + ask_prc) / 2),
volatility_lag1 = lag(volatility)
)
# Fill Size (no lag, trade-specific)
classification.data <- classification.data %>%
mutate(
fill_size = fill_qty * fill_prc # do not encode buy/sell sign
)
# Deviation from Mean Inventory Balance (lagged)
mean_inventory_balance <- mean(classification.data$balance, na.rm = TRUE)
classification.data <- classification.data %>%
mutate(
deviation_from_mean_balance_lag1 = balance_lag1 - mean_inventory_balance,
deviation_from_mean_balance_diff_lag = c(NA, diff(deviation_from_mean_balance_lag1))
)
# Deviation from Mean Market Price (lagged)
mean_mid_price <- mean(classification.data$mid_price, na.rm = TRUE)
classification.data <- classification.data %>%
mutate(
deviation_from_mean_mid_price_lag1 = mid_price_lag1 - mean_mid_price,
deviation_from_mean_mid_price_diff = c(NA, diff(deviation_from_mean_mid_price_lag1))
)
# Lagged Trade PnL and Cumulative PnL
classification.data <- classification.data %>%
mutate(
trade_pnl_lag1 = lag(trade_pnl),
cumulative_pnl_lag1 = lag(cumulative_pnl)
)
classification.data <- na.omit(classification.data) # Remove resulting NAs
str(classification.data)
numerical.variables <- c(
"volatility_lag1",
"spread_lag1",
"fill_prc",
"fill_qty",
"trade_pnl_lag1",
"cumulative_pnl_lag1",
"fee",
"balance_lag1",
"mid_price_lag1",
"fill_size",
"deviation_from_mean_balance_diff_lag",
"deviation_from_mean_mid_price_lag1",
"liquidity_Taker"
)
target.variables <- c("side")
classification.features <- unique(c(numerical.variables, target.variables))
classification.variables <- as.data.frame(classification.data[, ..numerical.variables])
classification.target <- classification.data %>% select(all_of(target.variables))
classification.selected <- cbind(
classification.variables,
classification.target
)
# Initial 80/20 split (train+val vs. test)
train_val_index <- createDataPartition(classification.selected$side, p = 0.8, list = FALSE)
train_val_data <- classification.selected[train_val_index, ]
test_data <- classification.selected[-train_val_index, ]
# Split 80% further into training (75%) and validation (25%) -> results in 60/20/20 overall
train_index <- createDataPartition(train_val_data$side, p = 0.75, list = FALSE)
train_data <- train_val_data[train_index, ]
val_data <- train_val_data[-train_index, ]
cat("Training Set Size:", nrow(train_data), "\n")
cat("Validation Set Size:", nrow(val_data), "\n")
cat("Test Set Size:", nrow(test_data), "\n")
# Remove known multicollinear features
classification.features <- setdiff(
classification.features,
c("mid_price_lag1", "balance_lag1")
)
# Separate predictors from the target
classification.predictors <- setdiff(classification.features, "side")
cat("Target Variable:", "side\n")
cat("Predictor Variables: ")
cat(paste(classification.predictors, collapse = ", "))
evaluate_model <- function(feature_set, method_name = NA) {
formula_str <- paste("side ~", paste(feature_set, collapse = " + "))
formula_obj <- as.formula(formula_str)
model <- glm(formula_obj, data = train_data, family = "binomial")
probs <- predict(model, newdata = test_data, type = "response")
roc_obj <- suppressMessages(roc(test_data$side, probs))
auc_val <- as.numeric(roc_obj$auc)
opt_thresh <- coords(roc_obj, x = "best", ret = "threshold", transpose = FALSE)
preds <- ifelse(probs > opt_thresh$threshold, 1, 0)
preds <- factor(preds, levels = levels(test_data$side))
cm <- confusionMatrix(preds, test_data$side)
return(data.frame(
subset = paste(feature_set, collapse = " + "),
num_vars = length(feature_set),
aic = AIC(model),
bic = BIC(model),
accuracy = cm$overall["Accuracy"],
balanced_accuracy = cm$byClass["Balanced Accuracy"],
roc_auc = auc_val,
stringsAsFactors = FALSE
))
}
best.subset.results <- data.frame(
subset = character(),
num_vars = integer(),
aic = numeric(),
bic = numeric(),
accuracy = numeric(),
balanced_accuracy = numeric(),
roc_auc = numeric(),
stringsAsFactors = FALSE
)
# Exhaustive subset search using combinations
for (k in 1:length(classification.predictors)) {
combos <- combinations(
n = length(classification.predictors), r = k,
v = classification.predictors
)
for (i in 1:nrow(combos)) {
vars <- combos[i, ]
result_row <- evaluate_model(vars)
best.subset.results <- rbind(best.subset.results, result_row)
}
}
best_by_aic <- best.subset.results %>%
arrange(aic) %>%
slice(1)
best_by_bic <- best.subset.results %>%
arrange(bic) %>%
slice(1)
best_by_balacc <- best.subset.results %>%
arrange(desc(balanced_accuracy)) %>%
slice(1)
best_by_auc <- best.subset.results %>%
arrange(desc(roc_auc)) %>%
slice(1)
print_model_summary <- function(title, row) {
cat(paste0(title, "\n"))
cat("  Formula: side ~", row$subset, "\n")
cat("  Num Vars:", row$num_vars, "\n")
cat("  Balanced Accuracy:", round(row$balanced_accuracy, 4), "\n")
cat("  Accuracy:", round(row$accuracy, 4), "\n")
cat("  AIC:", round(row$aic, 2), "\n")
cat("  BIC:", round(row$bic, 2), "\n")
cat("  ROC AUC:", round(row$roc_auc, 4), "\n\n")
}
print_model_summary("Best Subset Model by AIC", best_by_aic)
print_model_summary("Best Subset Model by BIC", best_by_bic)
print_model_summary("Best Subset Model by Balanced Accuracy", best_by_balacc)
print_model_summary("Best Subset Model by ROC AUC", best_by_auc)
x_logistic <- as.matrix(train_data[, classification.predictors])
y_logistic <- train_data$side
lasso_logistic <- cv.glmnet(
x = x_logistic, y = y_logistic,
family = "binomial", alpha = 1, nfolds = 10
)
best_lambda_logistic <- lasso_logistic$lambda.min
lasso_coef <- coef(lasso_logistic, s = best_lambda_logistic)
selected_lasso_features <- rownames(lasso_coef)[which(lasso_coef != 0)]
selected_lasso_features <- setdiff(selected_lasso_features, "(Intercept)")
lasso_summary <- evaluate_model(selected_lasso_features, "LASSO")
print_model_summary("LASSO Feature Selection", lasso_summary)
rfe_ctrl <- rfeControl(functions = caretFuncs, method = "cv", number = 10)
rfe_model <- suppressWarnings(
rfe(
x = train_data[, classification.predictors],
y = train_data$side,
sizes = 1:length(classification.predictors),
rfeControl = rfe_ctrl,
method = "glm",
family = "binomial"
)
)
selected_rfe_features <- predictors(rfe_model)
rfe_summary <- evaluate_model(selected_rfe_features, "RFE")
print_model_summary("RFE Feature Selection", rfe_summary)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(comment = FALSE)
knitr::opts_chunk$set(echo = TRUE)
plot(y = city$latitude, x = longitude)
